{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kn206/LLM_projects/blob/main/Copy_of_Chatwithpdf_advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOANrREUt1KD",
        "outputId": "ac72dc87-dd61-43d7-ad30-b847b1090123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hugchat in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: frontend in /usr/local/lib/python3.10/dist-packages (0.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.14)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.317)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hugchat) (2.31.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from hugchat) (1.0.0)\n",
            "Requirement already satisfied: starlette>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (0.27.0)\n",
            "Requirement already satisfied: uvicorn>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from frontend) (0.23.2)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.1.2)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from frontend) (23.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.104.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.8.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.46)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->hugchat) (2.0.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (0.18.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (11.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install  hugchat  frontend transformers  pypdf2 chromadb  langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Configure our variables\n",
        "- ie Authentication essentials.\n",
        "- it's from a temporary dummy email registered a hugging face account. Don't mind about it."
      ],
      "metadata": {
        "id": "cKzUTMKz5ZEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"EMAIL\"] = \"moteho4731@locawin.com\"\n",
        "os.environ[\"PASS\"] = \"Wambugu71?\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "5RLyXKHlwyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's read the data from a pdf.\n",
        "- Then create a vector database, here I used chromadb.\n",
        "- Then created embeddings using hugging face because it's free and open source.\n",
        "- Then used `test.pdf` as a sample to read the data (text) from, in this case I used a past paper in stocastic processes unit."
      ],
      "metadata": {
        "id": "CZfDyKS85wlv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAXuaHt7t6ih",
        "outputId": "5ef68f5a-5c73-44bf-9572-03e9497ab08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n",
            "WARNING:huggingface_hub.inference_api:You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
          ]
        }
      ],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n",
        "import warnings\n",
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "reader = PdfReader('/content/test2.pdf')\n",
        "\n",
        "#page = reader.pages[0]\n",
        "text = []\n",
        "for page in reader.pages:\n",
        "    text.append(page.extract_text())\n",
        "    #print(page.extract_text())\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "repo_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceHubEmbeddings(\n",
        "            repo_id=repo_id,\n",
        "            task=\"feature-extraction\"\n",
        "\n",
        "        )\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.create_documents(text)\n",
        "#random_str = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n",
        "db = Chroma.from_documents(texts, embeddings)\n",
        "#text = page.extract_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  creating a Custom LLM from hugchat library. hugchat is unofficial hugging face API.\n",
        "> I redesigned it to create a custom large language models inform of `langchains` format."
      ],
      "metadata": {
        "id": "_KLI9lTryqYR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_f6oO-JvwbX"
      },
      "outputs": [],
      "source": [
        "#The script has been made by wambugu kinyua.\n",
        "#access the huggingchat api\n",
        "#its a custom wrapper for a langchain llm\n",
        "import time\n",
        "from typing import Any, List, Mapping, Optional\n",
        "from hugchat import hugchat\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class llama2_wambugu_custom_chat(LLM):\n",
        "\n",
        "    \"\"\"HuggingChat LLM wrapper.\"\"\"\n",
        "\n",
        "    chatbot : Optional[hugchat.ChatBot] = None\n",
        "\n",
        "    email : Optional[str] = None\n",
        "    psw : Optional[str] = None\n",
        "    web_search: Optional[bool]= False\n",
        "    temperature: Optional[float] = 0.1\n",
        "    top_p: Optional[float] = 0.65\n",
        "    repetition_penalty: Optional[float] = 1.2\n",
        "    top_k: Optional[int]= 50\n",
        "    truncate: Optional[int] = 1000\n",
        "    watermark: Optional[bool] = False\n",
        "    max_new_tokens: Optional[int] = 1024\n",
        "    stop: Optional[list] = [\"</s>\"]\n",
        "    return_full_text: Optional[bool] = False,\n",
        "  #  stream: Optional[bool] = False,\n",
        "    _stream_yield_all: Optional[bool] = False\n",
        "    use_cache: Optional[bool] = False\n",
        "    is_retry: Optional[bool] = False\n",
        "    retry_count: Optional[int] = 5\n",
        "    chatbot : Optional[hugchat.ChatBot] = None\n",
        "  #  conversation: Optional[conversation] = None\n",
        "    cookie_path : Optional[str] = None\n",
        "\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"Custom llm for llama2 HuggingChat api. Made by wambugu kinyua 🤠🥳\"\n",
        "\n",
        "\n",
        "    def create_chatbot(self) -> None:\n",
        "        if not any([self.email, self.psw, self.cookie_path]):\n",
        "            raise ValueError(\"email, psw, or cookie_path is required.\")\n",
        "\n",
        "        try:\n",
        "            if self.email and self.psw:\n",
        "\n",
        "                from hugchat.login import Login\n",
        "\n",
        "                sign = Login(self.email, self.psw)\n",
        "                cookies = sign.login()\n",
        "\n",
        "            else:\n",
        "                cookies = self.cookie_path and hugchat.ChatBot(cookie_path=self.cookie_path)\n",
        "\n",
        "            self.chatbot = cookies.get_dict() and hugchat.ChatBot(cookies=cookies.get_dict())\n",
        "        except Exception as e:\n",
        "            raise ValueError(\"Login failed. Please check your email and password \" + str(e))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        if stop:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "\n",
        "        self.create_chatbot() if not self.chatbot else None\n",
        "\n",
        "        try:\n",
        "#You can change the model\n",
        "# index 0 for llama2 70b, 1 fo codellama, 2 for falcon 180b , 3 for Mistral 7b Ai\n",
        "            self.chatbot.switch_llm(3)\n",
        "            self.chatbot.new_conversation(switch_to=True)\n",
        "\n",
        "            resp = self.chatbot.query(\n",
        "                prompt, temperature=self.temperature,\n",
        "        top_p= self.top_p,\n",
        "        repetition_penalty = self.repetition_penalty,\n",
        "        top_k = self.top_k,\n",
        "        truncate= self.truncate,\n",
        "        watermark= self.watermark,\n",
        "        max_new_tokens = self.max_new_tokens,\n",
        "        stop = self.stop,\n",
        "        return_full_text = self.return_full_text,\n",
        "       # stream = self.stream,\n",
        "        _stream_yield_all = self._stream_yield_all,\n",
        "        use_cache = self.use_cache,\n",
        "        is_retry = self.is_retry,\n",
        "        retry_count = self.retry_count,\n",
        "\n",
        "        )\n",
        "            return resp['text']\n",
        "\n",
        "        except Exception as e:\n",
        "            raise ValueError(\"ChatBot failed, please check your parameters. \" + str(e))\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        params = {\"web_search\" :self.web_search,\n",
        "\"temperature\": self.temperature,\n",
        "\"top_p\": self.top_p,\"repetition_penalty\" : self.repetition_penalty,\n",
        "\"top_k\" : self.top_k,\"truncate\" :self.truncate,\"watermark\" : self.watermark,\"max_new_tokens\" : self.max_new_tokens,\n",
        "\"stop\" : self.stop,\"return_full_text\" : self.return_full_text,\"_stream_yield_all\" : self._stream_yield_all,\n",
        "\"use_cache\" : self.use_cache,\"is_retry\"  : self.is_retry, \"retry_count\" : self.retry_count }\n",
        "\n",
        "        return params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* let's test if the LLM is working."
      ],
      "metadata": {
        "id": "-BqyY8Lay71Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "fBrCm3dmtrlo",
        "outputId": "9880b589-66a0-48aa-bfe9-e5c9db788358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_getitem_\n",
            "done\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "\n",
        "\n",
        "llm = llama2_wambugu_custom_chat(email= os.environ[\"EMAIL\"],psw = os.environ[\"PASS\"])\n",
        "llm(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's query from our pdf."
      ],
      "metadata": {
        "id": "Guq8_YrGzDKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "eaAYb4e98eBc",
        "outputId": "cc27ce34-8eb2-48af-cafb-0b79b7c0866a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_getitem_\n",
            "done\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sure! Here's my answer for Question 2 part (a):\\n\\nTo find the probability that no vehicles are in the petrol station, we need to calculate the probability that all customers have left before another vehicle arrives. Since the arrival rate is 20 customers per hour and there are 60 minutes in an hour, the expected time between arrivals is 1/20 hours or 3 minutes. Therefore, after 3 minutes, it is likely that all customers will have left the petrol station.\\n\\nThe probability that all customers leave within 3 minutes can be calculated as follows:\\nP(all customers leave in 3 minutes) = e^(-λ \\\\* t) \\\\* λ^n / n!\\nwhere λ is the arrival rate (which is also the service rate), t is the time interval (in this case, 3 minutes), and n is the total number of customers in the system.\\n\\nSince we don't know the exact number of customers in the system, let's assume that there are N customers in the petrol station initially. Then, the probability that all customers leave within 3 minutes can be expressed as:\\nP(all customers leave in 3 minutes) = e^(-N \\\\* λ \\\\* 3) \\\\* N^N / N!\\n\\nIf we want to find the probability that no vehicles are in the petrol station, we need to sum up the probabilities for all possible values of N. However, since the number of customers in the system is not known, we cannot compute the exact probability. Instead, we can approximate the probability by assuming that the number of customers in the system is large enough so that the probability of having fewer than N customers is negligible. For example, if we assume that N = 100, then the probability of having fewer than N customers is approximately:\\nP(no customers) = e^(-100 \\\\* λ \\\\* 3) \\\\* 100^100 / 100!\\n\\nNote that this approximation assumes that the arrival rate remains constant over time, which may not be true in practice. Additionally, the accuracy of the approximation depends on the value of N and the actual arrival rate.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "retriever = db.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "#qa(\"what is data quality\")\n",
        "res = qa({\"query\": \"Answer for me question 2 a \"})#ask your question from your pdf\n",
        "res[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. The Ocr part\n",
        "- Remember to change the filename according to your photos file name after uploading it.\n",
        "- This is also a custom API that is free for OCR ie optical character Recognition😁.\n",
        "- you can request 500 times a day under a single IP address."
      ],
      "metadata": {
        "id": "HnlPBV1pyfMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "\n",
        "os.environ[\"API_KEY\"] = \"K89635879588957\"\n",
        "\n",
        "def ocr_space_file(filename, overlay=False, language='eng'):\n",
        "\n",
        "\n",
        "    payload = {'isOverlayRequired': overlay,\n",
        "               'apikey': os.environ[\"API_KEY\"],\n",
        "               'language': \"eng\",\n",
        "               }\n",
        "    with open(filename, 'rb') as f:\n",
        "        response = requests.post('https://api.ocr.space/parse/image',\n",
        "                          files={filename: f},\n",
        "                          data=payload,\n",
        "                          )\n",
        "    return response\n",
        "\n",
        "print(\"Processesing.......\")\n",
        "\n",
        "#text = ocr_space_file(filename='test1.jpg', language='eng')\n",
        "results = ocr_space_file(filename='test3.jpg', language='eng').json()\n",
        "#results = text[\"ParsedResults\"][0][\"ParsedText\"]\n",
        "res1 = results['ParsedResults'][0]['ParsedText']\n",
        "#[0][\"ParsedText\"])\n",
        "print(res1)"
      ],
      "metadata": {
        "id": "KE2JttRqybUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9216c23-ed27-4f47-e8e7-91902ec392a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processesing.......\n",
            "3:05:50...\n",
            "Queuing Mod...\n",
            "LTE .111\n",
            "4G 102\n",
            ".lll\n",
            "7.3 Characteristics of Queuing System\n",
            "In designing a good queuing system, it is necessary to have a good information about the\n",
            "model. The characteristics/features listed below would provide sufficient information.\n",
            "i).\n",
            "ii).\n",
            "iii).\n",
            "l).\n",
            "11).\n",
            "111).\n",
            "IV).\n",
            "Input (Arrival Pattern),\n",
            "The service mechanism,\n",
            "The queue discipline,\n",
            "The number of service channels.\n",
            "The input describe the manner in which the units arrive and join the system. The\n",
            "system may have a limited or unlimited capacity for holding units. A unit may arrive\n",
            "either singly or in a batch.\n",
            "The arrival time for any customer is unpredictable. Therefore, the arrival time and\n",
            "the number of customers arriving at any specified time intervals are usually random\n",
            "variables. A Poisson distribution of arrivals correspond to arrivals at random. In\n",
            "Poisson distribution, successive customers arrive after intervals which independently\n",
            "are and exponentially distributed. The Poisson distribution is important, as it is a\n",
            "suitable mathematical model of many practical queuing systems as describcxl by the\n",
            "parameter \"the average arrival rate\" .\n",
            "The service mechanism is a denscription of resources required for service. If there are\n",
            "infinite number of servers, then there will be no queue. If the number of servers is\n",
            "finite, then the customers are served according to a specific order. The time taken to\n",
            "serve a particular customer is called the service time. The time is a statistical\n",
            "variable and can be studied either as the number of services in a given period\n",
            "of time or the completion period of a service.\n",
            "The queue discipline indicate the way in which units form a queue and are serviced.\n",
            "The most common queuing discipline is First Come First Served (FCFS) or \"First-in,\n",
            "First-out\" (FIFO). Other disciplines include \"Last In First Out\" (LIFO) where last\n",
            "customer is serviced first, \"Service In Random Order\" (SIRO) in which the customers\n",
            "are randomly irrespective of their arrivals. \"Priority service\" is when the\n",
            "customers are grouped in priority classes based on urgency. \"Preemptive Priority\" is\n",
            "the highest priority given to the customer who enters into the service, immediately,\n",
            "even if a customer with lower priority is in sen•ice. \"Non-preemptive priority\" is where\n",
            "the customer goes ahead in the queue, but will be served only after the completion of\n",
            "the current service.\n",
            "Thc system may have a single channel or S-parallel channels for The more the\n",
            "number of service channels in the service facility, the greater the overall service rate of\n",
            "the facility. The combination of arrival rate and service rate is critical for determining\n",
            "the number of service channels. When there are a number of service channels available\n",
            "for service, then the arrangement of service depends upon the design of the system's\n",
            "service mechanism.\n",
            "Example\n",
            "In each of the following situations, identify the basic elements of the queuing model (i.e\n",
            "customer, server, service discipline, queue).\n",
            "89\n",
            "i). Planes taking off in an airport.\n",
            "Solution: ( traffic controller, Service disc\n",
            "waiting for signal to take off.)\n",
            "ii). Cars waiting at a stop light.\n",
            "Solution: (Customer=Cars, Server=Stop light, Service discipline= FCFS, Queue=Line\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the text from the image above."
      ],
      "metadata": {
        "id": "LFF202De5Ifh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts_ = text_splitter.create_documents([res1])\n",
        "db = Chroma.from_documents(texts_, embeddings)\n",
        "retriever2 = db.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever2)\n",
        "#qa(\"what is data quality\")\n",
        "res = qa({\"query\": \"Examples?\"})#ask your question from your pdf\n",
        "print(res[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMFKlkV_39H5",
        "outputId": "cb8c2881-b309-43fc-9ba2-49a12e7d29a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_getitem_\n",
            "done\n",
            " Sure! Here are some examples of queuing models:\n",
            "\n",
            "1. M/M/N Model: This model represents a manufacturing process where multiple machines work together to produce goods. Customers represent jobs that need to be processed, while servers represent machines that perform tasks such as cutting, painting, or assembly. The arrival rate represents the rate at which new jobs come in, while the service rate represents the speed at which each machine processes its job. The number of machines (servers) determines how many jobs can be processed simultaneously.\n",
            "\n",
            "2. G/G/N Model: This model represents a transportation network where vehicles travel along routes to pick up and deliver packages. Customers represent packages that need to be transported, while servers represent vehicles that move along the routes. The arrival rate represents the rate at which new packages arrive, while the service rate represents the speed at which each vehicle travels along the route. The number of vehicles (servers) determines how many packages can be picked up and delivered simultaneously.\n",
            "\n",
            "3. M/M/K Model: This model represents a hospital emergency room where patients arrive seeking medical attention. Customers represent patients, while servers represent doctors and nurses who treat them. The arrival rate represents the rate at which new patients arrive, while the service rate represents the speed at which each doctor or nurse treats a patient. The number of doctors and nurses (servers) determines how many patients can be treated simultaneously.\n",
            "\n",
            "4. D/D/N Model: This model represents a retail store where customers shop for products. Customers represent shoppers, while servers represent cashiers who ring up purchases. The arrival rate represents the rate at which new customers enter the store, while the service rate represents the speed at which each cashier rings up a purchase. The number of cashiers (servers) determines how many customers can be served simultaneously.\n",
            "\n",
            "I hope these examples help illustrate the different types of queuing models and their applications.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}