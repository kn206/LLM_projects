{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MusicCaps Explorer\n\nIn this notebook, we see how you can use `yt-dlp` to download clips from the MusicCaps dataset from Google. The MusicCaps dataset contains music and their associated text captions. You could use a dataset such as this to train a nice text-to-audio generation model ðŸ˜‰!\n\nThis notebook is 100% inspired and based on https://github.com/nateraw/download-musiccaps-dataset, with some additional annotations, so please give a star to that repo. The notebook shows how to load the dataset, underlying clips, and explore them.\n\nLet's get started! ðŸ”¥","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Introduction and setup\n\nLet's kick things off by installing some dependencies and load the dataset. **Note that Kaggle comes with an old datasets version but we need a newer one, so you might need to restart the notebook after install to make sure it's using the last version.**","metadata":{}},{"cell_type":"code","source":"%%capture\n! pip install -U datasets[audio]\n! pip install yt-dlp\n\n# For the interactive interface we'll need gradio\n! pip install gradio","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:11:04.508403Z","iopub.execute_input":"2023-01-31T14:11:04.509844Z","iopub.status.idle":"2023-01-31T14:11:39.620411Z","shell.execute_reply.started":"2023-01-31T14:11:04.509793Z","shell.execute_reply":"2023-01-31T14:11:39.618862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use the Hugging Face `datasets` library to load the dataset version hosted over there in the [google/MusicCaps](https://huggingface.co/datasets/google/MusicCaps) repository. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset('google/MusicCaps', split='train')\nds","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:09:50.319895Z","iopub.execute_input":"2023-01-31T14:09:50.32104Z","iopub.status.idle":"2023-01-31T14:09:55.109896Z","shell.execute_reply.started":"2023-01-31T14:09:50.320996Z","shell.execute_reply":"2023-01-31T14:09:55.108798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there are 5,521 music samples. Each sample contains information such as the audio caption and a YouTube ID, which can be surprising. Rather than exposing the audio files directly, this dataset contains the ID of YouTube videos (`ytid` field) and the `start_s` and `end_s`, which indicate the time range of the video of the sample. This makes it a bit harder to work compared to other datasets.","metadata":{}},{"cell_type":"markdown","source":"## Loading audio data\n\nAs our goal is just loading some data and exploring it, we'll limit ourselves to load only 32 samples. Feel free to change the `samples_to_load` variable in the next cell, but take into account that it might take a long time for the whole dataset.Kaggle notebooks have 4 cores, so we can use that for our advantage too. \n\nLet's go and download the data! ðŸš€","metadata":{}},{"cell_type":"code","source":"# JUST HELPER METHODS IN THIS CELL \n\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef download_clip(\n    video_identifier,\n    output_filename,\n    start_time,\n    end_time,\n    tmp_dir='/tmp/musiccaps',\n    num_attempts=5,\n    url_base='https://www.youtube.com/watch?v='\n):\n    status = False\n\n    command = f\"\"\"\n        yt-dlp --quiet --no-warnings -x --audio-format wav -f bestaudio -o \"{output_filename}\" --download-sections \"*{start_time}-{end_time}\" {url_base}{video_identifier}\n    \"\"\".strip()\n\n    attempts = 0\n    while True:\n        try:\n            output = subprocess.check_output(command, shell=True,\n                                                stderr=subprocess.STDOUT)\n        except subprocess.CalledProcessError as err:\n            attempts += 1\n            if attempts == num_attempts:\n                return status, err.output\n        else:\n            break\n\n    # Check if the video was successfully saved.\n    status = os.path.exists(output_filename)\n    return status, 'Downloaded'\n\ndef process(example):\n    outfile_path = str(data_dir / f\"{example['ytid']}.wav\")\n    status = True\n    if not os.path.exists(outfile_path):\n        status = False\n        status, log = download_clip(\n            example['ytid'],\n            outfile_path,\n            example['start_s'],\n            example['end_s'],\n        )\n\n    example['audio'] = outfile_path\n    example['download_status'] = status\n    return example","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:11:39.623587Z","iopub.execute_input":"2023-01-31T14:11:39.624125Z","iopub.status.idle":"2023-01-31T14:11:39.636708Z","shell.execute_reply.started":"2023-01-31T14:11:39.624073Z","shell.execute_reply":"2023-01-31T14:11:39.635233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Audio\n\nsamples_to_load = 32      # How many samples to load\ncores = 4                 # How many processes to use for the loading\nsampling_rate = 44100     # Sampling rate for the audio, keep in 44100\nwriter_batch_size = 1000  # How many examples to keep in memory per worker. Reduce if OOM.\ndata_dir = \"./music_data\" # Where to save the data\n\n# Just select some samples \nds = ds.select(range(samples_to_load))\n\n# Create directory where data will be saved\ndata_dir = Path(data_dir)\ndata_dir.mkdir(exist_ok=True, parents=True)\n\nds = ds.map(\n        process,\n        num_proc=cores,\n        writer_batch_size=writer_batch_size,\n        keep_in_memory=False\n    ).cast_column('audio', Audio(sampling_rate=sampling_rate))","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:11:39.638907Z","iopub.execute_input":"2023-01-31T14:11:39.639749Z","iopub.status.idle":"2023-01-31T14:11:40.071658Z","shell.execute_reply.started":"2023-01-31T14:11:39.639699Z","shell.execute_reply":"2023-01-31T14:11:40.070073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Done! Let's look at the data of an example","metadata":{}},{"cell_type":"code","source":"ds[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:11:56.040616Z","iopub.execute_input":"2023-01-31T14:11:56.041182Z","iopub.status.idle":"2023-01-31T14:11:59.777704Z","shell.execute_reply.started":"2023-01-31T14:11:56.041135Z","shell.execute_reply":"2023-01-31T14:11:59.776333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting! Let's see what we have\n* The `audio` key maps to a dictionary that contains both the audio (`.wav`) file and the `numpy` array of the data already loaded, as well as the sampling rate\n* `is_audioset_eval` specifies if it's from the eval or train split\n* The `caption` field has the description of the audio: \"The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft female vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services.\"","metadata":{}},{"cell_type":"markdown","source":"## Interactive explorer\n\nWe can use [Gradio](https://gradio.app/), an open-source library to build ML demos, to build an interface in which the user selects the index of the sample and can then listen to the audio and read the caption. Gradio has a nice `Interface` class which has three key components\n* `inputs`: specifies which are the input components. In this case, we'll want a slider that will represent the index.\n* `outputs`: the output components. In this case, we want an audio and a textarea\n* Any inference function that receives the `inputs` type and outputs the `outputs` types. \n\nLet's see it in action!","metadata":{}},{"cell_type":"code","source":"import gradio as gr\n\ndef get_example(idx):\n    ex = ds[idx]\n    return ex['audio']['path'], ex['caption']\n\ngr.Interface(\n    get_example,\n    inputs=gr.Slider(0, len(ds) - 1, value=0, step=1),\n    outputs=['audio', 'textarea'],\n    allow_flagging=\"never\",\n    live=True\n).launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T14:20:02.18242Z","iopub.execute_input":"2023-01-31T14:20:02.182953Z","iopub.status.idle":"2023-01-31T14:20:07.693352Z","shell.execute_reply.started":"2023-01-31T14:20:02.182898Z","shell.execute_reply":"2023-01-31T14:20:07.692374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it! I hope you find this notebook useful! \n\nHugs!ðŸ¤—","metadata":{}}]}